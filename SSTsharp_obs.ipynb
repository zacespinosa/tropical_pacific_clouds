{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import xcdat as xc\n",
    "import xsearch as xs\n",
    "import xskillscore as xscore\n",
    "\n",
    "from glob import glob \n",
    "from global_land_mask import globe\n",
    "from scipy.stats import linregress\n",
    "from scipy.linalg import lstsq\n",
    "from typing import List, Tuple, Dict, Union, Optional, Any, Callable, Iterable, Sequence, cast\n",
    "\n",
    "# Ignore xarray warnings (bad practice)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\") \n",
    "\n",
    "# Define Constants\n",
    "prctl_bins = np.arange(0,105,5) # bottom edge of percentile bins\n",
    "prctl_binmids = (prctl_bins[:-1]+prctl_bins[1:])/2 # center of percentile bins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 16,        # Global font size\n",
    "    'axes.labelsize': 16,   # Font size for axis labels\n",
    "    'axes.titlesize': 16,   # Font size for subplot titles\n",
    "    'legend.fontsize': 12,  # Font size for legend\n",
    "    'xtick.labelsize': 12,  # Font size for x-axis tick labels\n",
    "    'ytick.labelsize': 12   # Font size for y-axis tick labels\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_percentile_figures(ds: xr.Dataset, model: str) -> None:\n",
    "    time = ds.time\n",
    "    percentiles = ds.bins\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(30, 20), sharey=True)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    # Plot Binaverages\n",
    "    im = axes[0].pcolor(time,percentiles, ds.binavg.T, vmin=16,vmax=32) \n",
    "    axes[0].set_title(\"Average SST in each percentile bin\", loc=\"left\")\n",
    "    cbar = fig.colorbar(im, ax=axes[0])\n",
    "    cbar.set_label('K')\n",
    "\n",
    "    # Plot Binanomalies\n",
    "    im = axes[1].pcolor(time,percentiles, ds.binanom.T, vmin=-1,vmax=1, cmap='RdBu_r')\n",
    "    twin_ax1 = axes[1].twinx()\n",
    "    twin_ax1.plot(time, ds.mean_anom, color='black', label=\"average\", linewidth=4)\n",
    "    cbar = fig.colorbar(im, ax=axes[1])\n",
    "    axes[1].set_title(\"SST Anomaly from bin average over climatology\", loc=\"left\")\n",
    "    twin_ax1.legend()\n",
    "    cbar.set_label('K')\n",
    "\n",
    "    # Plot Binanomalies Bar\n",
    "    im = axes[2].pcolor(time,percentiles, ds.binanom_bar.T, vmin=-1,vmax=1, cmap='RdBu_r')\n",
    "    axes[2].set_title(\"SST Anomalies relative to tropical average\", loc=\"left\")\n",
    "    twin_ax2 = axes[2].twinx()\n",
    "    twin_ax2.plot(time, (ds.sharp - ds.flat), color='black', label=\"SST# - SSTb\", linewidth=4)\n",
    "    twin_ax2.hlines(np.nanmean(ds.sharp - ds.flat), xmin=time[0], xmax=time[-1], color=\"gray\", linestyle=\"--\", linewidth=4) \n",
    "    twin_ax2.legend()\n",
    "    cbar = fig.colorbar(im, ax=axes[2])\n",
    "    cbar.set_label('K')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_ylabel('SST percentile')\n",
    "        ax.set_xlabel('Time')\n",
    "\n",
    "    axes[0].set_title(model, fontsize=30, fontweight=\"bold\", loc=\"right\")\n",
    "\n",
    "    # Save figure to folder figures as pdf\n",
    "    plt.savefig(f\"figures/{model}_percentiles.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/{model}_percentiles.png\", bbox_inches='tight')\n",
    "\n",
    "# create_percentile_figures(ds, model=\"CESM2_test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWCRE plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_swcre_percentile_figures(ds: xr.Dataset, model: str) -> None:\n",
    "    time = ds.time\n",
    "    percentiles = ds.bins\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(30, 20), sharey=True)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    # Plot Binaverages\n",
    "    im = axes[0].pcolor(time,percentiles, ds.binavg.T) #, vmin=-120, vmax=0)\n",
    "    axes[0].set_title(\"Average SWCRE in each percentile bin\", loc=\"left\")\n",
    "    cbar = fig.colorbar(im, ax=axes[0])\n",
    "    cbar.set_label(r'$Wm^{-2}$')\n",
    "\n",
    "    # Plot Binanomalies\n",
    "    im = axes[1].pcolor(time,percentiles, ds.binanom.T, cmap='RdBu_r', vmax=10, vmin=-10)\n",
    "    twin_ax1 = axes[1].twinx()\n",
    "    twin_ax1.plot(time, ds.mean_anom, color='black', label=\"average\", linewidth=4)\n",
    "    cbar = fig.colorbar(im, ax=axes[1])\n",
    "    axes[1].set_title(\"SWCRE Anomaly from bin average over climatology\", loc=\"left\")\n",
    "    twin_ax1.legend()\n",
    "    cbar.set_label(r'$Wm^{-2}$')\n",
    "\n",
    "    # Plot Binanomalies Bar\n",
    "    im = axes[2].pcolor(time,percentiles, ds.binanom_bar.T, cmap='RdBu_r', vmax=10, vmin=-10)\n",
    "    axes[2].set_title(\"SWCRE Anomalies relative to tropical average\", loc=\"left\")\n",
    "    twin_ax2 = axes[2].twinx()\n",
    "    twin_ax2.plot(time, (ds.sharp - ds.flat), color='black', label=\"SWCRE# - SWCREb\", linewidth=4)\n",
    "    twin_ax2.hlines(np.nanmean(-(ds.sharp - ds.flat)), xmin=time[0], xmax=time[-1], color=\"gray\", linestyle=\"--\", linewidth=4) \n",
    "    twin_ax2.legend()\n",
    "    cbar = fig.colorbar(im, ax=axes[2])\n",
    "    cbar.set_label(r'$Wm^{-2}$')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_ylabel('SWCRE sorted on SST percentile')\n",
    "        ax.set_xlabel('Time')\n",
    "\n",
    "    axes[0].set_title(model, fontsize=30, fontweight=\"bold\", loc=\"right\")\n",
    "\n",
    "    # Save figure to folder figures as pdf\n",
    "    plt.savefig(f\"figures/{model}_SWCRE_percentiles.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/{model}_SWCRE_percentiles.png\", bbox_inches='tight')\n",
    "\n",
    "# create_percentile_figures(ds, model=\"CESM2_test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST Trend plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_rolling_trend(window: int, sharp: xr.DataArray, flat: xr.DataArray, trend: xr.DataArray, raw: xr.DataArray, model: str) -> None:\n",
    "    # Create matplotlib 2 subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(30, 20))\n",
    "    ax1_twin = ax1.twinx()\n",
    "    ntime = sharp.shape[0]\n",
    "    time = np.arange(ntime)\n",
    "\n",
    "    # Add grid lines to all subplots\n",
    "    ax1.grid(); ax2.grid(); ax3.grid()\n",
    "\n",
    "    # Plot time vs trend on ax1\n",
    "    ax1.plot(time, raw, color='black', label=r\"SST$d$\", linewidth=4)\n",
    "\n",
    "    # Plot sharp on ax1_twin\n",
    "    ax1_twin.plot(time, flat, color='purple', label=r\"SST$b$\", linewidth=4)\n",
    "    ax1_twin.plot(time, sharp, color='maroon', label=r\"SST#\", linewidth=4)\n",
    "    ax1.set_ylabel(r\"SST$d$ (K)\"); ax1_twin.set_ylabel(r\"SST# and SST$b$ (K)\")\n",
    "    ax1.set_title(r\"SST$d$ (SST# - SST$b$)\", loc=\"left\", fontweight=\"bold\")\n",
    "    # Set x tick labels starting in 1850 every 5 years \n",
    "    ax1.set_xticklabels(np.arange(1850, 1850+int(ntime/12), 10))\n",
    "    ax1.set_xticks(time[::12*10])\n",
    "    # Add a black horizontal line at 0 \n",
    "    ax1.hlines(0, xmin=time[0], xmax=time[-1], color=\"black\", linestyle=\"--\", linewidth=4)\n",
    "    # Retrieve the handles and labels from both axes\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax1_twin.get_legend_handles_labels()\n",
    "    # Concatenate the handles and labels from both axes\n",
    "    handles = handles1 + handles2\n",
    "    labels = labels1 + labels2\n",
    "    # Create the legend using the concatenated handles and labels\n",
    "    ax1.legend(handles, labels)\n",
    "    \n",
    "    # Set ylim of twin_ax1 to be 20% larger than the range of the data\n",
    "    ax1_twin.set_ylim(np.nanmin(raw)*1.2, np.nanmax(raw)*1.2)\n",
    "    ax1.set_ylim(np.nanmin(raw)*1.2, np.nanmax(raw)*1.2)\n",
    "\n",
    "    # Plot ntime vs trend on ax2\n",
    "    # trend_time = np.arange(int(window/2), ntime - int(window/2))\n",
    "    ntrend = trend.shape[0]\n",
    "    ax2.plot(np.arange(ntrend), trend, color='black', label=\"trend\", linewidth=4)\n",
    "    yrs_window = window/(12*2) # half the years of the length of the window\n",
    "    xticks = np.arange(0, ntrend, 10)\n",
    "    xticklabels = np.arange(1850 + yrs_window, 1850 + yrs_window + ntrend, 10) \n",
    "    ax2.set_xticks(xticks)\n",
    "    ax2.set_xticklabels(xticklabels)\n",
    "    ax2.set_xlim(0, ntrend)\n",
    "\n",
    "    ax2.set_title(\"SST Trend (K/30yr)\", loc=\"left\", fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"SST Trend (K/30yr)\"); ax2.set_xlabel(\"Time\")\n",
    "    ax2.hlines(0, xmin=0, xmax=ntrend, color=\"black\", linestyle=\"--\", linewidth=4)\n",
    "\n",
    "    hist = sns.histplot(trend, ax=ax3, kde=True, stat=\"density\", linewidth=0)\n",
    "    ax3.set_title(r\"SST$d$ Trend (K/30yr)\", loc=\"left\", fontweight=\"bold\")\n",
    "    ax3.set_title(f\"mean: {np.mean(trend):.2f}, variance: {np.var(trend):.2f}\", loc=\"right\", fontweight=\"bold\")\n",
    "    # Add a vertical line at the mean of the trend and the variance and have the ymin and ymax be the max of the histogram\n",
    "    ax3.vlines(np.mean(trend), ymin=0, ymax=np.max(hist.get_lines()[0].get_data()[1]), color=\"black\", linestyle=\"--\", linewidth=4)\n",
    "    fig.suptitle(model, fontsize=30, fontweight=\"bold\", y=0.95)\n",
    "\n",
    "    # Save figure to folder figures as pdf\n",
    "    plt.savefig(f\"figures/{model}_trend.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/{model}_trend.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWCF plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_swcf(swcf: xr.DataArray, model: str = \"\"):\n",
    "    \"\"\"Plot the SWCF\n",
    "\n",
    "    Args:\n",
    "        swcf (xr.DataArray): SWCF\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1, figsize=(5,3))\n",
    "\n",
    "    ax.plot(swcf.bins, swcf.values, label=model, linewidth=4)\n",
    "    ax.legend()\n",
    "    ax.set_title(\"SWCF\", loc=\"left\", fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Percentile\")\n",
    "    ax.set_ylabel(r\"SWCF [$Wm^{-2}K^{-1}$]\")\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.grid()\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "    # Save figure\n",
    "    plt.savefig(f\"figures/{model}_SWCF.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/{model}_SWCF.png\", bbox_inches='tight')\n",
    "\n",
    "# plot_swcf(swcf, model=\"CESM2_test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST Sharp Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Library to compute weighted quantiles, including the weighted median, of\n",
    "numpy arrays.\n",
    "\n",
    "https://github.com/nudomarinero/wquantiles\n",
    "\"\"\"\n",
    "def quantile_1D(data, weights, quantile):\n",
    "    \"\"\"\n",
    "    Compute the weighted quantile of a 1D numpy array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        Input array (one dimension).\n",
    "    weights : ndarray\n",
    "        Array with the weights of the same size of `data`.\n",
    "    quantile : float\n",
    "        Quantile to compute. It must have a value between 0 and 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    quantile_1D : float\n",
    "        The output value.\n",
    "    \"\"\"\n",
    "    # Check the data\n",
    "    if not isinstance(data, np.matrix):\n",
    "        data = np.asarray(data)\n",
    "    if not isinstance(weights, np.matrix):\n",
    "        weights = np.asarray(weights)\n",
    "    nd = data.ndim\n",
    "    if nd != 1:\n",
    "        raise TypeError(\"data must be a one dimensional array\")\n",
    "    ndw = weights.ndim\n",
    "    if ndw != 1:\n",
    "        raise TypeError(\"weights must be a one dimensional array\")\n",
    "    if data.shape != weights.shape:\n",
    "        raise TypeError(\"the length of data and weights must be the same\")\n",
    "    if ((quantile > 1.) or (quantile < 0.)):\n",
    "        raise ValueError(\"quantile must have a value between 0. and 1.\")\n",
    "    # Sort the data\n",
    "    ind_sorted = np.argsort(data)\n",
    "    sorted_data = data[ind_sorted]\n",
    "    sorted_weights = weights[ind_sorted]\n",
    "    # Compute the auxiliary arrays\n",
    "    Sn = np.cumsum(sorted_weights)\n",
    "    # TODO: Check that the weights do not sum zero\n",
    "    #assert Sn != 0, \"The sum of the weights must not be zero\"\n",
    "    Pn = (Sn-0.5*sorted_weights)/Sn[-1]\n",
    "    # Get the value of the weighted median\n",
    "    return np.interp(quantile, Pn, sorted_data)\n",
    "\n",
    "\n",
    "# def get_percentiles(data,wts,bins):\n",
    "#     # wrapper for previous functions\n",
    "#     quantiles = bins/100\n",
    "#     return np.quantile(data, quantiles)\n",
    "\n",
    "def get_percentiles(data,wts,bins):\n",
    "    # wrapper for previous functions\n",
    "    quantiles = bins/100\n",
    "    newbins=[]\n",
    "    # if len(np.where(np.isnan(data))[0]) > 0:\n",
    "        # raise Exception\n",
    "\n",
    "    # return np.quantile(data, quantiles)\n",
    "    for q in quantiles:\n",
    "        newbins.append(quantile_1D(data, wts, q))\n",
    "    return newbins\n",
    "\n",
    "# Testing\n",
    "# bins = get_percentiles(list(np.random.rand((100))) + list([np.nan, np.nan, np.nan]), np.ones(103), bins=np.arange(0, 110, 10))\n",
    "# bins = get_percentiles(np.random.rand((100)), np.ones(100), bins=np.arange(0, 110, 10))\n",
    "# bins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST Sharp Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_SSTsharp_sorting(da: xr.DataArray, prctl_bins: np.ndarray, prctl_binmids: np.ndarray, weights: np.ndarray) -> xr.Dataset:\n",
    "    \"\"\"The goal of this function is to calculate SSTsharp as defined in Fueglistaler (2019)\n",
    "\n",
    "        Observational evidence for two modes of coupling between sea surface temperatures, \n",
    "        tropospheric temperature profile, and shortwave cloud radiative effect in the tropics. \n",
    "        Geophysical Research Letters, 46, 9890â€“9898. https://doi.org/10.1029/2019GL083990\n",
    "\n",
    "    Args:\n",
    "        ds (xr.DataArray): _description_\n",
    "        nbins (int, optional): _description_. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        ds (xr.Dataset): containing the following xr.DataArrays:\n",
    "            bingavg (xr.DataArray)[time, percentile]: average temperature in each percentile bin with quantile defined at that time step\n",
    "            binanom (xr.DataArray)[time, percentile]: average temperature in each percentile bin minus the monthly climatology\n",
    "            sortIdxs (xr.DataArray)[time, lon]: Indexes of the sorted data that can be used to sort other dataX (e.g. swcre, rsut, rsutcs) using dataY (e.g. SST)\n",
    "            binanom_bar (xr.DataArray)[time, percentile]: binanoms minus the tropical average SST at that time step\n",
    "            mean_anom (xr.DataArray)[time, percentile]: tropical average SST anomaly at that time step\n",
    "            sharp (xr.DataArray)[time]: The average of binanom_bar over the warmest 30% at each time step\n",
    "            flat (xr.DataArray)[time]:  The average of binanom_bar over the coldest 30% at each time step \n",
    "\n",
    "    \"\"\"\n",
    "    ntime, nlat, nlon = da.shape\n",
    "    nbins = len(prctl_bins)\n",
    "\n",
    "    binavg = np.zeros((ntime,nbins))\n",
    "    binwts = np.zeros((ntime,nbins))\n",
    "    sortIdxs = np.zeros((ntime,nlon*nlat))\n",
    "    not_nans = np.zeros((ntime,nlon*nlat))\n",
    "    not_nans.fill(np.nan)\n",
    "    sortIdxs.fill(np.nan)\n",
    "\n",
    "    for i in range(ntime):\n",
    "        # print(da.isel(time=i).lat.values, da.isel(time=i).lon.values)\n",
    "        da_slice = da.isel(time=i).to_numpy().flatten()\n",
    "        wts_slice = weights[i,:,:].to_numpy().flatten()\n",
    "        # print(\"da_slice shape: \", da_slice.shape)\n",
    "        # print(\"wts_slice shape: \", wts_slice.shape)\n",
    "\n",
    "        # Remove nans\n",
    "        v = np.where(~np.isnan(da_slice))[0] \n",
    "        not_nans[i,:len(v)] = v\n",
    "        wts_slice = wts_slice[v]\n",
    "        da_slice = da_slice[v]\n",
    "        # print(\"da_slice shape after nans removed: \", da_slice.shape)\n",
    "        # print(\"wts_slice shape after nans removed: \", wts_slice.shape)\n",
    "\n",
    "        # Calculate percentiles (these are the values separating the quantiles)\n",
    "        bins = get_percentiles(da_slice, wts_slice, prctl_bins)\n",
    "        inds = np.digitize(da_slice, bins)\n",
    "        sortIdxs[i,:len(da_slice)] = inds\n",
    "        # print(\"sortIdxs : \", inds.shape)\n",
    "        # print(np.max(inds), np.min(inds), inds[:20])\n",
    "        # print(wts_slice[:20])\n",
    "\n",
    "        for j, b in enumerate(np.arange(nbins)): # note that bin 0 only has a single (max) value\n",
    "            numer = np.nansum(da_slice[np.where(inds==b)]*wts_slice[np.where(inds==b)])\n",
    "            denom = np.nansum(wts_slice[np.where(inds==b)])\n",
    "            binavg[i,j] = numer/denom\n",
    "            binwts[i,j] = denom\n",
    "\n",
    "        # print(binavg[i,:])\n",
    "        # print(binwts[i,:])\n",
    "\n",
    "    # Deseasonalize the data -- remove climatology computed over entire timeseries:\n",
    "    binanom = np.zeros((ntime,nbins))\n",
    "    for m in range(12):\n",
    "        binanom[m::12,:] = binavg[m::12,:] - np.nanmean(binavg[m::12, :], 0)\n",
    "    \n",
    "    # Time slice SST Tropical mean\n",
    "    TMean = np.nansum(binanom*binwts,axis=1) / np.nansum(binwts,axis=1)\n",
    "    # Calculate binanom_bar\n",
    "    binanom_bar = binanom - TMean[:, None]\n",
    "    \n",
    "    # Calculate SSTsharp\n",
    "    sharpThreshold = np.where(prctl_bins >= 70)[0]\n",
    "    sharp = np.nansum(binanom_bar[:,sharpThreshold]*binwts[:,sharpThreshold], axis=1) / np.nansum(binwts[:, sharpThreshold], axis=1)\n",
    "    # Calculate SSTflat\n",
    "    flatThreshold = np.where(prctl_bins <= 30)[0]\n",
    "    flat = np.nansum(binanom_bar[:,flatThreshold]*binwts[:,flatThreshold], axis=1) / np.nansum(binwts[:, flatThreshold], axis=1)  \n",
    "\n",
    "    # Create Dataset => remove first element that is nan for all bins\n",
    "    ds = xr.Dataset({\n",
    "        'binavg':(('time','bins'), binavg[:, 1:]),\n",
    "        'binanom': (('time','bins'), binanom[:, 1:]), \n",
    "        'sortIdxs': (('time','idx'), sortIdxs), \n",
    "        'not_nans': (('time','idx'), not_nans), \n",
    "        'binanom_bar': (('time','bins'), binanom_bar[:, 1:]),\n",
    "        'binwts':(('time','bins'), binwts[:, 1:]),\n",
    "        'mean_anom':(('time'), TMean),\n",
    "        'sharp':(('time'), sharp),\n",
    "        'flat':(('time'), flat)\n",
    "    },\n",
    "        coords={\n",
    "            'time': da.time, \n",
    "            'bins': prctl_binmids.data,\n",
    "            'idx': np.arange(nlat*nlon),\n",
    "        }\n",
    "    ) \n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWCRE Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_SSTsharp_to_SWCRE(da: xr.DataArray, prctl_bins: np.ndarray, ds_sharp: xr.Dataset, weights: np.ndarray, model: str) -> xr.Dataset:\n",
    "    \"\"\"Apply SSTsharp sorting to SWCRE\n",
    "\n",
    "    Args:\n",
    "        da (xr.DataArray): swcre raw monthly data\n",
    "        prctl_bins (np.ndarray): percentiles to use for sorting from ds_sharp\n",
    "        ds_sharp (xr.Dataset): SST sharp package\n",
    "        model (str): name of model\n",
    "\n",
    "    Returns:\n",
    "        ds (xr.Dataset): containing the following xr.DataArrays:\n",
    "            bingavg (xr.DataArray)[time, percentile]: average SWCRE in each percentile bin with quantile defined at that time step\n",
    "            binanom (xr.DataArray)[time, percentile]: average SWCRE in each percentile bin minus the monthly climatology\n",
    "            binanom_bar (xr.DataArray)[time, percentile]: binanoms minus the tropical average SWCRE at that time step\n",
    "            mean_anom (xr.DataArray)[time, percentile]: tropical average SST anomaly at that time step\n",
    "            sharp (xr.DataArray)[time]: The average of binanom_bar over the warmest 30% at each time step\n",
    "            flat (xr.DataArray)[time]:  The average of binanom_bar over the coldest 30% at each time step \n",
    "    \"\"\"\n",
    "    ntime, _, _ = da.shape\n",
    "    nbins = len(prctl_bins)\n",
    "    \n",
    "    binavg = np.zeros((ntime,nbins))\n",
    "    binwts = np.zeros((ntime,nbins))\n",
    "\n",
    "    for i in range(ntime):\n",
    "        # Get current time slice\n",
    "        # print(da.isel(time=i).lat.values, da.isel(time=i).lon.values)\n",
    "        da_slice = da.isel(time=i).to_numpy().flatten()\n",
    "        wts_slice = weights[i, :, :].to_numpy().flatten()\n",
    "        # print(\"da_slice shape: \", da_slice.shape)\n",
    "        # print(\"wts_slice shape: \", wts_slice.shape)\n",
    "       \n",
    "        # Remove values where corresponding SST data is nan\n",
    "        sst_non_nans = ds_sharp[\"not_nans\"].isel(time=i).values\n",
    "        sst_non_nans = np.array(sst_non_nans[~np.isnan(sst_non_nans)], dtype=int)\n",
    "        da_slice = da_slice[sst_non_nans]\n",
    "        wts_slice = wts_slice[sst_non_nans]\n",
    "        # print(\"da_slice shape after nans removed: \", da_slice.shape)\n",
    "        # print(\"wts_slice shape after nans removed: \", wts_slice.shape)\n",
    "\n",
    "        # Remove filler nans from sorted inds\n",
    "        inds = ds_sharp[\"sortIdxs\"].isel(time=i).values\n",
    "        inds = inds[np.where(~np.isnan(inds))] \n",
    "        # print(\"sortIdxs shape: \", inds.shape)\n",
    "\n",
    "        # Remove nans from SWCRE \n",
    "        nanMask = np.where(~np.isnan(da_slice))[0]\n",
    "        wts_slice = wts_slice[nanMask]\n",
    "        da_slice = da_slice[nanMask]\n",
    "        inds = np.array(inds[nanMask], dtype=int)\n",
    "        # print(\"da_slice shape after nans removed: \", da_slice.shape)\n",
    "        # print(\"wts_slice shape after nans removed: \", wts_slice.shape)\n",
    "        # print(np.max(inds), np.min(inds), inds[:20])\n",
    "        # print(wts_slice[:20])\n",
    "\n",
    "        # Calculate percentiles (these are the values separating the quantiles)\n",
    "        # bins = get_percentiles(da_slice, wts_slice, prctl_bins)\n",
    "        # inds = np.digitize(da_slice, bins)\n",
    "\n",
    "        for j, b in enumerate(np.arange(nbins)): # note that bin 0 only has a single (max) value\n",
    "            numer = np.nansum(da_slice[np.where(inds==b)]*wts_slice[np.where(inds==b)])\n",
    "            denom = np.nansum(wts_slice[np.where(inds==b)])\n",
    "            binavg[i,j] = numer/denom\n",
    "            binwts[i,j] = denom\n",
    "        \n",
    "        # print(binavg[i,:])\n",
    "        # print(binwts[i,:])\n",
    "\n",
    "    # Deseasonalize the data -- remove climatology computed over entire timeseries:\n",
    "    binanom = np.zeros((ntime,nbins))\n",
    "    for m in range(12):\n",
    "        binanom[m::12,:] = binavg[m::12,:] - np.nanmean(binavg[m::12, :], 0)\n",
    "    # binanom[:,:] = binavg[:,:] - np.nanmean(binavg[:, :], 0)\n",
    "    \n",
    "    # Time slice SWCRE Tropical mean\n",
    "    TMean = np.nansum(binanom*binwts,axis=1) / np.nansum(binwts,axis=1)\n",
    "    # Calculate binanom_bar\n",
    "    binanom_bar = binanom - TMean[:, None]\n",
    "\n",
    "    # Calculate SWCREsharp\n",
    "    sharpThreshold = np.where(prctl_bins >= 70)[0]\n",
    "    sharp = np.nansum(binanom_bar[:,sharpThreshold]*binwts[:,sharpThreshold], axis=1) / np.nansum(binwts[:, sharpThreshold], axis=1)\n",
    "    # Calculate SWCREflat\n",
    "    flatThreshold = np.where(prctl_bins <= 30)[0]\n",
    "    flat = np.nansum(binanom_bar[:,flatThreshold]*binwts[:,flatThreshold], axis=1) / np.nansum(binwts[:, flatThreshold], axis=1)  \n",
    "\n",
    "    # Create Dataset => remove first element that is nan for all bins\n",
    "    ds = xr.Dataset({\n",
    "        'binavg':(('time','bins'), binavg[:, 1:]),\n",
    "        'binanom': (('time','bins'), binanom[:, 1:]), \n",
    "        'binanom_bar': (('time','bins'), binanom_bar[:, 1:]),\n",
    "        'binwts':(('time','bins'), binwts[:, 1:]),\n",
    "        'mean_anom':(('time'), TMean),\n",
    "        'sharp':(('time'), sharp),\n",
    "        'flat':(('time'), flat)\n",
    "    },\n",
    "        coords={\n",
    "            'time': da.time, \n",
    "            'bins': prctl_binmids,\n",
    "        }\n",
    "    ) \n",
    "    \n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SST Rolling Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_rolling_trend(data: xr.Dataset, model: str = \"\", window: int = 12*30, step: int = 12) -> xr.Dataset:\n",
    "    \"\"\"Calculate the rolling gradient of a dataset\n",
    "\n",
    "    Args:\n",
    "        data (xr.Dataset): _description_\n",
    "        window (int, optional): _description_. Defaults to 120*30.\n",
    "        step (int, optional): _description_. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        np.array: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate SSTd = SSTsharp (warm) - SSTflat (cold)\n",
    "    SSTd = data[\"sharp\"] - data[\"flat\"]\n",
    "    ntime = SSTd.shape[0]\n",
    "\n",
    "    SSTd_trend = np.zeros(int(np.ceil(np.ceil((ntime - window)/12))))\n",
    "    SSTd_trend.fill(np.nan)\n",
    "\n",
    "    # Calculate rolling trend of SSTd\n",
    "    time_idx = xr.DataArray(np.arange(window), dims=(\"time\"))\n",
    "    for j, i in enumerate(range(0, ntime - window, step)):\n",
    "        slice_SSTd = SSTd.isel(time=slice(i, int(i+window)))\n",
    "        SSTd_trend[j] = xscore.linslope(time_idx, slice_SSTd, dim='time', skipna=True).values\n",
    "    \n",
    "    # Convert SSTd trend and SSTd to xr.DataArray\n",
    "    SSTd_trend = xr.DataArray(SSTd_trend, dims=(\"time\"), coords={\"time\": np.arange(SSTd_trend.shape[0])})\n",
    "    # Convert SSTd to xr.DataArray\n",
    "    SSTd = xr.DataArray(SSTd, dims=(\"time\"), coords={\"time\": np.arange(SSTd.shape[0])})\n",
    "    # Combine SSTd and SSTd_trend into xr.Dataset\n",
    "    ds = xr.Dataset({\"SSTd\": SSTd, \"SSTd_trend\": SSTd_trend})\n",
    "    # Sanity plot of SSTd and SSTd_trend\n",
    "    # sanity_rolling_trend(window=window, sharp=data[\"sharp\"], flat=data[\"flat\"], trend=SSTd_trend*window, raw=SSTd, model=model) \n",
    "\n",
    "    # Return SSTd and SSTd_trend\n",
    "    return ds\n",
    "\n",
    "# calculate_rolling_trend(ds[\"CESM2\"], model=\"CESM2_test\", window=12, step=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SWCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_swcf_percentiles(SSTsharp: xr.Dataset, SWCREsharp: xr.Dataset) -> xr.Dataset:\n",
    "    sst_anoms = SSTsharp[\"binanom\"]\n",
    "    swcre_anoms = SWCREsharp[\"binanom\"]\n",
    "    swcf = xscore.linslope(sst_anoms, swcre_anoms, dim='time', skipna=True)\n",
    "    return swcf\n",
    "\n",
    "# swcf = calculate_swcf_percentiles(SSTsharp=ds[\"CESM2\"], SWCREsharp=swcre)\n",
    "# swcf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 1800, 24, 144)\n",
      "(54, 1800, 24, 144)\n",
      "(40, 1800, 24, 144)\n",
      "(41, 1800, 24, 144)\n"
     ]
    }
   ],
   "source": [
    "# Load cmip6 SST data\n",
    "# tos = xr.open_dataset(f\"data/tos_mon_1850-2100_CMIP5_piControl.nc\", chunks=\"auto\")\n",
    "def get_sharp_data(tos: xr.Dataset, swcre: xr.Dataset) -> Tuple[xr.Dataset, xr.Dataset]:\n",
    "    tos = tos.bounds.add_bounds(\"X\")\n",
    "    tos = tos.bounds.add_bounds(\"Y\")\n",
    "    tos = xc.swap_lon_axis(tos, to=(-180, 180))\n",
    "\n",
    "    # Load cmip6 swcre data\n",
    "    swcre = swcre.bounds.add_bounds(\"X\")\n",
    "    swcre = swcre.bounds.add_bounds(\"Y\")\n",
    "    swcre = xc.swap_lon_axis(swcre, to=(-180, 180))\n",
    "\n",
    "    # Set land to NaN\n",
    "    lon_grid,lat_grid = np.meshgrid(tos.lon,tos.lat)\n",
    "    globe_land_mask = globe.is_land(lat_grid,lon_grid)\n",
    "    globe_land_mask_nd = np.tile(globe_land_mask,(tos[\"tos\"].shape[1],1,1))\n",
    "    # plt.contourf(tos.lon, tos.lat, globe_land_mask_nd[0]) # Sanity check plot\n",
    "    tos_no_land = xr.where(globe_land_mask_nd==True,np.nan,tos[\"tos\"]) \n",
    "    swcre_no_land = xr.where(globe_land_mask_nd==True,np.nan,swcre[\"swcre\"]) \n",
    "\n",
    "    # Get Area Weights:\n",
    "    wt = tos.spatial.get_weights(axis=[\"Y\", \"X\"])\n",
    "    AW = (wt/wt.sum()) # summing this over lat and lon = 1\n",
    "    _, area_wts = xr.broadcast(tos.time,AW)\n",
    "    weights = xr.where(globe_land_mask_nd==True,0,area_wts)    \n",
    "    # plt.contourf(tos.lon, tos.lat, weights[0]); plt.colorbar() # Sanity check plot\n",
    "\n",
    "    # FOCUS ON TROPICAL PACIFIC FOR SST SHARP CALCULATIONS\n",
    "    # pacific = (list(np.arange(-180, -110)) + list(np.arange(110, 180)))\n",
    "    # tos_no_land = tos_no_land.sel(lon=pacific, method=\"nearest\")\n",
    "    # swcre_no_land = swcre_no_land.sel(lon=pacific, method=\"nearest\")\n",
    "\n",
    "    # FOCUS ON TROPICS FOR SST SHARP CALCULATIONS\n",
    "    tos_tropics = tos_no_land.sel(lat=slice(-30,30))\n",
    "    swcre_tropics = swcre_no_land.sel(lat=slice(-30,30))\n",
    "    weights_tropics = weights.sel(lat=slice(-30,30))\n",
    "\n",
    "    # Replace 0s with nans: for some reason, swcre has 0 in many places -> this may be physical (no clouds), but lets just see what happens when altering\n",
    "    # swcre_tropics = xr.where(swcre_tropics == 0, np.nan, swcre_tropics)\n",
    "\n",
    "    swcre_tropics.load()\n",
    "    tos_tropics.load()\n",
    "\n",
    "    print(tos_tropics.shape)\n",
    "    print(swcre_tropics.shape)\n",
    "    \n",
    "    return tos_tropics, swcre_tropics, weights_tropics\n",
    "\n",
    "# Load CMIP6 \n",
    "tos_cmip6 = xr.open_dataset(f\"data/tos_mon_1850-2100_CMIP6_piControl.nc\", chunks=\"auto\")\n",
    "swcre_cmip6 = xr.open_dataset(\"data/swcre_cmip6_monthly_1850-2000.nc\")\n",
    "tos_tropics_cmip6, swcre_tropics_cmip6, weights_tropics_cmip6 = get_sharp_data(tos_cmip6, swcre_cmip6)\n",
    "\n",
    "# Load CMIP5\n",
    "tos_cmip5 = xr.open_dataset(f\"data/tos_mon_1850-2100_CMIP5_piControl.nc\", chunks=\"auto\")\n",
    "swcre_cmip5 = xr.open_dataset(\"data/swcre_cmip5_monthly_1850-2000.nc\")\n",
    "tos_tropics_cmip5, swcre_tropics_cmip5, weights_tropics_cmip5 = get_sharp_data(tos_cmip5, swcre_cmip5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
